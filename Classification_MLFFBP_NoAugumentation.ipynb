{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52Dse9qCMcql"
   },
   "source": [
    "# **Code begins**\n",
    "\n",
    "**MLFFBP-scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BdrKVHSBIxtx"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for ensemble model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load not augmentated dataset from shi yuan\n",
    "X_train = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\train_fruit_img.npy\")\n",
    "Y_train = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\train_label_id.npy\")\n",
    "X_validate = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\validate_fruit_img.npy\")\n",
    "Y_validate = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\validate_label_id.npy\")\n",
    "\n",
    "X_test = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\test_fruit_img.npy\")\n",
    "Y_test = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\test_label_id.npy\")\n",
    "# training and validation set\n",
    "\n",
    "mean = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\meanfile.npy\")\n",
    "std = np.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\\\CA1\\\\without_augmentation\\\\stdfile.npy\")\n",
    "mean = mean.reshape(1,1,1,3)\n",
    "std = std.reshape(1,1,1,3)\n",
    "\n",
    "## Approach 2\n",
    "X_train = X_train.astype(np.float32, copy=False)\n",
    "X_validate = X_validate.astype(np.float32,copy = False)\n",
    "X_test = X_test.astype(np.float32,copy = False)\n",
    "\n",
    "X_train -= mean\n",
    "X_train /= std\n",
    "X_validate -= mean\n",
    "X_validate /= std\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "## flatten data\n",
    "X_flat_train = X_train.reshape(X_train.shape[0],32*32*3)\n",
    "X_flat_validate = X_validate.reshape(X_validate.shape[0],32*32*3)\n",
    "X_flat_test = X_test.reshape(X_test.shape[0],32*32*3)\n",
    "\n",
    "print('Original Sizes:', X_train.shape, X_validate.shape, Y_train.shape, Y_validate.shape,X_test.shape, Y_test.shape)\n",
    "print('Flattened:', X_flat_train.shape, X_flat_validate.shape, X_flat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', activation='relu', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(32*3,32),random_state=1, \n",
    "                    verbose = True, learning_rate_init=0.01, \n",
    "                    warm_start=True)\n",
    "\n",
    "clf.fit(X_flat_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set score: %f\" % clf.score(X_flat_train,Y_train))\n",
    "print(\"validate set score: %f\" % clf.score(X_flat_validate,Y_validate))\n",
    "print(\"validate set score: %f\" % clf.score(X_flat_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick and save the best model\n",
    "N_iter = 30\n",
    "clf1 = MLPClassifier(max_iter=1,solver='sgd', activation='relu', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(32*3,32),random_state=1, \n",
    "                    verbose = True, learning_rate_init=0.01, \n",
    "                    warm_start=True)\n",
    "train_score = []\n",
    "validate_score = []\n",
    "Best_score = 0\n",
    "Best_epoch = 0\n",
    "Bestmodel = \"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\test.sav\"\n",
    "for iter in range(0,N_iter):\n",
    "  clf1.fit(X_flat_train,Y_train)\n",
    "  train_score_new = clf1.score(X_flat_train,Y_train)\n",
    "  validate_score_new = clf1.score(X_flat_validate,Y_validate)\n",
    "  if validate_score_new > Best_score:\n",
    "    Best_score = validate_score_new\n",
    "    Best_epoch = iter\n",
    "    joblib.dump(clf1,Bestmodel)    \n",
    "  train_score.append(train_score_new)\n",
    "  validate_score.append(validate_score_new)\n",
    "  \n",
    "np.save(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\Best_epoch.npy\",Best_epoch)\n",
    "np.save(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\train_score.npy\",train_score)\n",
    "np.save(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\Best_score.npy\",Best_score)\n",
    "np.save(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\validate_score.npy\",validate_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot history for accuracy and loss\n",
    "def plot_model(model_details, train_acc_score, val_acc_score):\n",
    "    # Create sub-plots\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # Summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(train_acc_score)+1),train_acc_score)\n",
    "    axs[0].plot(range(1,len(val_acc_score)+1),val_acc_score)\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(train_acc_score)+1),len(train_acc_score)/10)\n",
    "    axs[0].legend(['train', 'validate'], loc='best')\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_details.loss_curve_)+1),model_details.loss_curve_)\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_details.loss_curve_)+1),len(model_details.loss_curve_)/10)\n",
    "    axs[1].legend(['train', 'validate'], loc='best')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot confusion matrix of prediction value\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, '',\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(clf1, train_score, validate_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"augmented model train set score: %f\" % clf1.score(X_flat_train,Y_train))\n",
    "print(\"augmented model validate set score: %f\" % clf1.score(X_flat_validate,Y_validate))\n",
    "print(\"augmented model test set score: %f\" % clf1.score(X_flat_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_load = joblib.load(\"D:\\\\NUS_ACADEMY\\\\00computer_intelligence\\\\CAs\\CA1\\\\without_augmentation\\\\Bestmodel_MLBP.sav\")\n",
    "print(\"original model train set score: %f\" % best_model_load.score(X_flat_train,Y_train))\n",
    "print(\"original model validate set score: %f\" % best_model_load.score(X_flat_validate,Y_validate))\n",
    "print(\"original model test set score: %f\" % best_model_load.score(X_flat_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "validation_MLFFBP.ipynb",
   "provenance": [
    {
     "file_id": "1tzSqYvgLhRz3YXURQUABeGqECVed4StV",
     "timestamp": 1526126473300
    },
    {
     "file_id": "1WndePtbpbGB7QbAQ-TmfP6SO3WlMezY6",
     "timestamp": 1525525787872
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
